{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5c769677c1e50ccf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# CBS Week 8 Assessment: Bayesian Concept Learning\n",
    "## Semester 2 2024\n",
    "\n",
    "\n",
    "This notebook is due on September 16th. Please make sure that your notebook validates before you submit it --- if your notebook doesn't validate the automated grader may run into issues.\n",
    "\n",
    "This notebook has two parts. The first part will have you use code to calculate the three rules of probability. The second part will explore a simple version of Josh Tenenbaum's number game. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7dcb6603dbe7390d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "suppressPackageStartupMessages({\n",
    "    library(tidyverse)\n",
    "    library(testthat)\n",
    "    library(knitr)\n",
    "    library(kableExtra)\n",
    "    library(IRdisplay)  \n",
    "})\n",
    "\n",
    "options(repr.plot.width=16, repr.plot.height=8)\n",
    "\n",
    "# a function for displaying tables\n",
    "show_table <- function(d) {\n",
    "    kable(d, \"html\", align=\"c\")  %>% \n",
    "        as.character()  %>% \n",
    "        display_html()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-513801355145a07f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "# Part One: Three Rules of Probability\n",
    "\n",
    "Here is a joint probability distribution $P(R, E, A)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-07a68c547d051f61",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       " <thead>\n",
       "  <tr>\n",
       "   <th style=\"text-align:center;\"> R </th>\n",
       "   <th style=\"text-align:center;\"> E </th>\n",
       "   <th style=\"text-align:center;\"> A </th>\n",
       "   <th style=\"text-align:center;\"> p_r_e_a </th>\n",
       "  </tr>\n",
       " </thead>\n",
       "<tbody>\n",
       "  <tr>\n",
       "   <td style=\"text-align:center;\"> 1 </td>\n",
       "   <td style=\"text-align:center;\"> 1 </td>\n",
       "   <td style=\"text-align:center;\"> 1 </td>\n",
       "   <td style=\"text-align:center;\"> 0.840 </td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "   <td style=\"text-align:center;\"> 1 </td>\n",
       "   <td style=\"text-align:center;\"> 1 </td>\n",
       "   <td style=\"text-align:center;\"> 2 </td>\n",
       "   <td style=\"text-align:center;\"> 0.010 </td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "   <td style=\"text-align:center;\"> 1 </td>\n",
       "   <td style=\"text-align:center;\"> 2 </td>\n",
       "   <td style=\"text-align:center;\"> 1 </td>\n",
       "   <td style=\"text-align:center;\"> 0.070 </td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "   <td style=\"text-align:center;\"> 1 </td>\n",
       "   <td style=\"text-align:center;\"> 2 </td>\n",
       "   <td style=\"text-align:center;\"> 2 </td>\n",
       "   <td style=\"text-align:center;\"> 0.030 </td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "   <td style=\"text-align:center;\"> 2 </td>\n",
       "   <td style=\"text-align:center;\"> 1 </td>\n",
       "   <td style=\"text-align:center;\"> 1 </td>\n",
       "   <td style=\"text-align:center;\"> 0.003 </td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "   <td style=\"text-align:center;\"> 2 </td>\n",
       "   <td style=\"text-align:center;\"> 1 </td>\n",
       "   <td style=\"text-align:center;\"> 2 </td>\n",
       "   <td style=\"text-align:center;\"> 0.040 </td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "   <td style=\"text-align:center;\"> 2 </td>\n",
       "   <td style=\"text-align:center;\"> 2 </td>\n",
       "   <td style=\"text-align:center;\"> 1 </td>\n",
       "   <td style=\"text-align:center;\"> 0.001 </td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "   <td style=\"text-align:center;\"> 2 </td>\n",
       "   <td style=\"text-align:center;\"> 2 </td>\n",
       "   <td style=\"text-align:center;\"> 2 </td>\n",
       "   <td style=\"text-align:center;\"> 0.006 </td>\n",
       "  </tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "p_r_e_a <- tibble(R = c(1,1,1,1,2,2,2,2), \n",
    "             E = c(1,1,2,2,1,1,2,2), \n",
    "             A = c(1,2,1,2,1,2,1,2), \n",
    "             p_r_e_a = c(0.84, 0.01, 0.07, 0.03, 0.003, 0.04, 0.001, 0.006) )\n",
    "\n",
    "show_table(p_r_e_a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6a0b9e2bb0ccf8b6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The first rule of probability is that distributions must sum to one.\n",
    "\n",
    "Let's check this is a valid probability distribution and sums to 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-88cacbc32aff1f23",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "1"
      ],
      "text/latex": [
       "1"
      ],
      "text/markdown": [
       "1"
      ],
      "text/plain": [
       "[1] 1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "p_r_e_a %>% \n",
    "    pull(p_r_e_a) %>% \n",
    "    sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d3bfeab444fbf07a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "The second rule of probability is how to marginalize over events.\n",
    "\n",
    "<center>\n",
    "Marginalization: <br> $P(h) = \\sum_d P(d, h)$\n",
    "</center>\n",
    "    \n",
    "### Exercise 1 (1 point)\n",
    "\n",
    "Let's compute the distribution $P(R, E)$ by marginalizing over $A$.\n",
    "\n",
    "To compute `p_r_e`, make a column called `p_r_e` that specifies the marginal distribution $P(R,E)$. The cleanest way to make the `p_r_e` column is to use `group_by()` then `summarise()`. Fix the `summarise()` statement below so that `p_r_e` is defined properly (currently it is set to a constant).\n",
    "\n",
    "You might get a `summarise()` grouping warning. You can safely ignore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`summarise()` has grouped output by 'R'. You can override using the `.groups` argument.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       " <thead>\n",
       "  <tr>\n",
       "   <th style=\"text-align:center;\"> R </th>\n",
       "   <th style=\"text-align:center;\"> E </th>\n",
       "   <th style=\"text-align:center;\"> p_r_e </th>\n",
       "  </tr>\n",
       " </thead>\n",
       "<tbody>\n",
       "  <tr>\n",
       "   <td style=\"text-align:center;\"> 1 </td>\n",
       "   <td style=\"text-align:center;\"> 1 </td>\n",
       "   <td style=\"text-align:center;\"> NA </td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "   <td style=\"text-align:center;\"> 1 </td>\n",
       "   <td style=\"text-align:center;\"> 2 </td>\n",
       "   <td style=\"text-align:center;\"> NA </td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "   <td style=\"text-align:center;\"> 2 </td>\n",
       "   <td style=\"text-align:center;\"> 1 </td>\n",
       "   <td style=\"text-align:center;\"> NA </td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "   <td style=\"text-align:center;\"> 2 </td>\n",
       "   <td style=\"text-align:center;\"> 2 </td>\n",
       "   <td style=\"text-align:center;\"> NA </td>\n",
       "  </tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "p_r_e <-  p_r_e_a %>%\n",
    "    group_by(R, E) %>% \n",
    "    summarise(p_r_e = NA) %>% # YOUR CODE HERE Fix this line\n",
    "    ungroup()\n",
    "\n",
    "show_table(p_r_e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-7be0a05df9dcf02f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error: p_r_e$p_r_e not equal to `answ`.\nModes: logical, numeric\ntarget is logical, current is numeric\n",
     "output_type": "error",
     "traceback": [
      "Error: p_r_e$p_r_e not equal to `answ`.\nModes: logical, numeric\ntarget is logical, current is numeric\nTraceback:\n",
      "1. expect_equal(p_r_e$p_r_e, answ)",
      "2. expect(comp$equal, sprintf(\"%s not equal to %s.\\n%s\", act$lab, \n .     exp$lab, comp$message), info = info)",
      "3. exp_signal(exp)",
      "4. withRestarts(if (expectation_broken(exp)) {\n .     stop(exp)\n . } else {\n .     signalCondition(exp)\n . }, continue_test = function(e) NULL)",
      "5. withOneRestart(expr, restarts[[1L]])",
      "6. doWithOneRestart(return(expr), restart)"
     ]
    }
   ],
   "source": [
    "# Hidden Tests Here\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "answ <-  p_r_e_a %>%\n",
    "    group_by(R, E) %>% \n",
    "    summarise(p_r_e = sum(p_r_e_a), .groups = 'drop')  %>%\n",
    "    ungroup() %>% pull(p_r_e)\n",
    "\n",
    "expect_equal(p_r_e$p_r_e, answ)\n",
    "\n",
    "### END HIDDEN TESTS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f61861c766e7e5a9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The third rule of proabilities is how to condition on events.\n",
    "\n",
    "<center>\n",
    "    Conditioning: <br>  $P(d|h) = \\frac{P(d, h)}{P(h)}$\n",
    "</center>\n",
    "\n",
    "### Exercise 2 (1 point)\n",
    "\n",
    "Now let's compute the conditional probability distribution $P(A|R, E)$. We'll use `p_a_given_r_e` as the variable name for this conditional probability distribution.\n",
    "\n",
    "As a first step, add a column to `d1` called `p_a_given_r_e` that captures the conditional distribution $P(a|r,e)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "p_r_e_a = p_r_e_a %>%\n",
    "    left_join(p_r_e) %>%\n",
    "    mutate(p_a_given_r_e = NA) %>%  # YOUR CODE HERE Fix this line\n",
    "    select(-p_r_e)\n",
    "\n",
    "show_table(p_r_e_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-67642ae752905b2d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Hidden Tests Here\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "answ <-  p_r_e_a %>%\n",
    "    left_join(p_r_e) %>%\n",
    "    mutate(p_a_given_r_e = p_r_e_a / p_r_e) %>%\n",
    "    select(-p_r_e) %>% pull(p_a_given_r_e)\n",
    "\n",
    "\n",
    "expect_equal(p_r_e_a$p_a_given_r_e, answ)\n",
    "\n",
    "### END HIDDEN TESTS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f11421c7fe9be3fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "Bayes' Rule uses both conditioning and marginalizing with the joint distribution between data and hypotheses $P(d,h)$.\n",
    "\n",
    "$$\n",
    "P(h|d) = \\frac{P(d,h)}{P(d)} = \\frac{P(d,h)}{\\sum_h P(d,h)}\n",
    "$$\n",
    "\n",
    "## Let's Try a Concrete Example \n",
    "\n",
    "Let's say you're texting your friend \"We need to talk.\" You want to estimate how long before they respond. Hopefully you could make a cuppa without leaving them waiting.\n",
    "\n",
    "We know that how long they take to respond depends on what they're doing. They might be ``attending`` to their phone. They might be busy ``multitasking``. They be taking a `powernap`. They might be attending to their phone but very reasonably think that a text like that should be followed by more details. So it's ``your turn`` to respond. Given each of these hypotheses, we can calculate the likelihood of a response over a 15 minute window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4db9d70a3ef137bc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "p_t_given_h = data.frame(time = seq(0, 15, 0.2)) %>%\n",
    "    mutate(attending = dgamma(time, shape=1, rate=2) / sum(dgamma(time, shape=1, rate=2)),\n",
    "           multitasking = dgamma(time, shape=1, rate=0.5) / sum(dgamma(time, shape=1, rate=0.5)),\n",
    "           yourturn = dgamma(time, shape=2, rate=2) / sum(dgamma(time, shape=2, rate=2)),\n",
    "           powernap = dgamma(time-9.5, shape=1, rate=1) / sum(dgamma(time-9.5, shape=1, rate=1))) %>%\n",
    "    gather(hypothesis, p_t_given_h, attending:powernap)\n",
    "\n",
    "p_t_given_h %>%\n",
    "    ggplot(aes(time, p_t_given_h)) +\n",
    "    facet_wrap(~hypothesis, scales='free_y') +\n",
    "    geom_bar(stat='identity') +\n",
    "    xlab('Time to reply (min)') +\n",
    "    ylab('Likelihood P(t|h)') +\n",
    "    theme_bw(base_size=24)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b8121d2b9b6c0e64",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "Now, in order to figure out how long you'll have to wait, we need to know the base rates. How often is your friend attending to their phone, multitasking or powernapping? How likely is that they're waiting for you to send more details? \n",
    "\n",
    "Now, every person has different experiences $D_{old}$ that will shape their base rates. In Bayes' rule, we normally represent the base rates as the prior $P(h)$. In reality, our priors today are conditioned on our past experiences $P(h|D_{old})$. \n",
    "\n",
    "You'll notice that today's priors are old posteriors. Thus, we can also write Bayes rule as:\n",
    "\n",
    "$$ P(h|\\{d, D_{old}\\}) = \\frac{P(d | h, D_{old}) P(h | D_{old}) }{P(d| D_{old}) }.$$\n",
    "\n",
    "\n",
    "Today, let's use my priors over these hypotheses given my experiences with friends:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a1398476d9484826",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "p_h = data.frame(hypothesis = c('attending', 'multitasking', 'powernap', 'yourturn'),\n",
    "          p_h = c(0.15, 0.75, 0.05, 0.05))\n",
    "\n",
    "show_table(p_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3a325963f5465f8d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 3 (1 point)\n",
    "\n",
    "Now that you have a prior and a likelihood. You can calculate the joint distribution $P(t,h)$ and evidence $P(t)$.\n",
    "\n",
    "In the following code block, marginalize over the hypothesis $h$ to estimate the response time probability $P(t)$. In the data.frame `p_t`, have a column for `time` and a column `p_t` for the probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "p_t = p_t_given_h %>%\n",
    "    left_join(p_h)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-683d2a6fd53c421f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "p_t %>%\n",
    "ggplot(aes(time, p_t)) +\n",
    "geom_bar(stat='identity') +\n",
    "xlab('Time to Reply (min)') +\n",
    "ylab('Posterior Probability P(t)') +\n",
    "theme_bw(base_size=24)\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "answ = p_t_given_h %>%\n",
    "    left_join(p_h) %>%\n",
    "    group_by(time) %>%\n",
    "    summarise(p_t = sum(p_t_given_h * p_h)) %>%\n",
    "    arrange(time) %>% pull(p_t)\n",
    "\n",
    "expect_equal(sum(answ), 1)\n",
    "expect_equal(answ, p_t %>% arrange(time) %>% pull(p_t))\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cb4a1661c4b0be81",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 4 (1 point)\n",
    "\n",
    "It turns out it took 1.0 minutes to respond.\n",
    "\n",
    "What is the most likely reason it took 1.0 minute to respond? Justify your answer using probability theory? **(Max Response Length: 3 sentences)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place any code you want here (or do the math by hand. I don't actually care.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a79e0f6629c7cd26",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "ANSWER HERE FOR THE POINT. REMEBER TO SAY WHY!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-98662f21770a15f5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Part Two: The Number Game\n",
    "\n",
    "The number game was covered in class this week, and is also described by the Murphy reading available on Canvas. In class we discussed a version of the number game that considered integers between 1 and 100. Here we'll make things even simpler and will consider only integers between 1 and 12.\n",
    "\n",
    "\n",
    "The number game involves inferences about a concept $C$. For example, if $C$ is the concept \"even numbers\", the extension of $C$ would be $\\{2,4,6,8,10,12\\}$. If $C$ is the concept \"numbers between 4 and 6\", then the extension would be $\\{4,5,6\\}$.  \n",
    "\n",
    "## Hypothesis space $\\mathcal{H}$ and priors $p(h)$\n",
    "\n",
    "We're going to consider four different priors and two different likelihood functions (strong and weak sampling). Combining these priors and likelihood functions produces a total of 8 different Bayesian models.\n",
    "\n",
    "Let's start by setting up the hypothesis space $\\mathcal{H}$ and the priors. Provided that you understand how the priors are defined, the code for setting up these priors is not that important for our purposes, so feel free to skim it.\n",
    "\n",
    "First we define two helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-52b6d9d7bc45dac5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# A helper function that sets bits in H_VEC (a 12 element vector) corresponding to H_MEMBERS, a set of \n",
    "# concept members \n",
    "\n",
    "set_member_indices <- function(h_vec, h_members) {\n",
    "  h_vec[h_members] <- 1\n",
    "  return(h_vec)\n",
    "}\n",
    "\n",
    "# ... and a helper function that converts a H_VEC to a string\n",
    "indices_to_str<- function(h_vec) {\n",
    "  paste0(which(h_vec==1), collapse=\"/\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-95bd81b8b51426dc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now define `h_math`, a hypothesis space including 8 mathematical concepts and a uniform prior (`prior_math`) over this hypothesis space. Eight mathematical concepts will be enough for our purposes but others could be included (e.g. \"powers of 2\", or \"cube numbers\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d2dd379edf080a10",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "n <- 12\n",
    "zeros <- replicate(n, 0)\n",
    "h_math <-\n",
    "  tibble(h_members = list(\n",
    "    c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12),   # all numbers\n",
    "    c(2, 4, 6, 8, 10, 12),   # even numbers\n",
    "    c(1, 3, 5, 7, 9, 11),    # odd numbers\n",
    "    c(3, 6, 9, 12),          # multiples of 3\n",
    "    c(4, 8, 12),             # multiples of 4\n",
    "    c(5, 10),                # multiples of 5\n",
    "    c(6, 12),                # multiples of 6\n",
    "    c(1, 4, 9)               # square numbers\n",
    "  ),\n",
    "  prior_math = 1) %>%\n",
    "  mutate(h_vec = map(h_members, ~set_member_indices(zeros, .x))) %>% \n",
    "  mutate(prior_math= prior_math/sum(prior_math)) %>% \n",
    "  mutate(h_str= map_chr(h_vec, indices_to_str)) %>% \n",
    "  select(h_str, prior_math) \n",
    "\n",
    "show_table(head(h_math))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e97bb4815220dabf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Next we'll define `h_interval`, a hypothesis space of interval concepts (e.g. \"numbers between 2 and 5\", \"numbers between 6 and 12\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d1feb0daf6d82099",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# enumerate all pairs (l, r) where l is less than r\n",
    "pairs <- t(combn(1:n, 2)) \n",
    "colnames(pairs) <- c(\"left\", \"right\")\n",
    "\n",
    "# a helper function that sets bits in H_VEC that include all numbers from LEFT to RIGHT \n",
    "pair_to_indices <- function(h_vec, left, right) {\n",
    "  h_vec[left:right] <- 1\n",
    "  return(h_vec)\n",
    "}\n",
    "\n",
    "h_interval <- tibble(left = 1:n, right = 1:n) %>% \n",
    "  bind_rows(as_tibble(pairs)) %>% \n",
    "  mutate(prior_interval = 1, h_vec = map2(left, right, ~pair_to_indices(zeros, .x, .y)))  %>% \n",
    "  mutate(prior_interval = prior_interval /sum(prior_interval)) %>% \n",
    "  mutate(h_str= map_chr(h_vec, indices_to_str)) %>% \n",
    "  select(h_str, prior_interval)\n",
    "\n",
    "# look at the last 5 hypotheses\n",
    "show_table(tail(h_interval, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e5d52a9bdf8a77e4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Finally, we'll set up a hypothesis space `h_all` that includes all possible extensions of concept $C$ along with a uniform prior `h_uniform` over this space. The complete set of hypotheses in `h_all` is the hypothesis space $\\mathcal{H}$ we'll be using for subsequent analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-532c572e38a8ce2b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "h_all <-  expand.grid(replicate(n, 0:1, simplify = FALSE))\n",
    "colnames(h_all) <- paste(\"V\", 1:n, sep=\"\")\n",
    "# drop the empty hypothesis\n",
    "h_all <- h_all[-1,]\n",
    "h_all <- tibble(h_all) %>% \n",
    "  rowwise() %>% \n",
    "  mutate(h_vec = list(c_across(V1:V12))) %>% \n",
    "  ungroup() %>% \n",
    "  mutate(h_str= map_chr(h_vec, indices_to_str)) %>%\n",
    "  mutate(prior_uniform = 1)  %>% \n",
    "  mutate(prior_uniform = 1/sum(prior_uniform)) \n",
    "\n",
    "# look at the first 5 hypotheses. You'll see some additional columns V1:V12 and h_list that will be used later for\n",
    "# computing likelihood functions and model generalizations. Column V1 indicates whether or not 1 belongs to the \n",
    "# concept, and similarly for V2 through V12. h_vec is a vector indicating which numbers belong to the concept.\n",
    "show_table(head(h_all, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bf9b6c2172df9788",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we'll combine `h_math`, `h_interval` and `h_all` to specify a single table `h` that specifies the entire hypothesis space $\\mathcal{H}$ along with four prior distributions. `prior_math` assigns nonzero probability only to the hypotheses in `h_math`, `prior_interval` assigns nonzero probability only to the hypotheses in `h_interval`, and `prior_uniform` assigns uniform probability to all possible hypotheses. The final prior `prior_combo` is a \"combination prior\" defined as the average of the three other priors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-007c39c26172982d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "h <- h_all %>% \n",
    "  left_join(h_math, by = \"h_str\") %>% \n",
    "  left_join(h_interval, by = \"h_str\") %>% \n",
    "  replace_na(list(prior_math = 0, prior_interval = 0)) %>% \n",
    "  mutate(prior_combo = (prior_math + prior_interval + prior_uniform) / 3 )\n",
    "\n",
    "show_table(head(h, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1a80fb16d9db0eaa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Likelihood function $p(X|h)$\n",
    "We're ready to define the likelihood function. Following the notation in the Murphy reading, let $X$ be a list of positive examples of the concept. For instance, if we are told that 4 and 8 belong to the concept, then $X$ would be the list $[4, 8]$. The likelihood function $P(X|h)$ specifies the probability of observing $X$ given that the true concept is captured by hypothesis $h$.\n",
    "\n",
    "We'll consider two likelihood functions. *Strong sampling* assumes that the observations in $X$ are randomly sampled from the set of all positive examples of the concept, which means that $p(X|h) = \\left[ \\frac{1}{|h|} \\right]^n$, where $|h|$ is the size of hypothesis $h$. \n",
    "\n",
    "*Weak sampling* assumes that the examples were chosen by some process independent of $h$ and subsequently labeled as positive (i.e. members of the concept) or negative (not members of the concept). Under weak sampling it is possible to observe negative examples, but we're only going to consider cases in which all examples happen to be positive. Weak sampling corresponds to the likelihood function $p(X|h) = 1$ if all observations in $X$ are consistent with $h$, and 0 otherwise.\n",
    "\n",
    "### Question 5 (1  point)\n",
    "The code below sets up the likelihood function, but the definition of strong sampling is incorrect. Fix the code so that strong sampling is defined correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H_VEC is a binary vector indicating which numbers belong to the concept, and X is a set of positive examples\n",
    "# ltype is a flag indicating whether we're assuming \"strong\" or \"weak\" sampling\n",
    "likelihood_fn <- function(h_vec, X, ltype) {\n",
    "  n <- length(X)\n",
    "  h_size <- sum(h_vec)\n",
    "  if ( sum(h_vec[X]) < n ) {\n",
    "    l <- 0\n",
    "  } else {\n",
    "    l <- switch(ltype,\n",
    "        strong = (1/h_size)^n, # FIX THIS LINE\n",
    "        weak  = 1)\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-9c1ad42d9afea548",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# we'll test likelihood_fn() by applying it to the observation set X = [1,9,11]. Make sure that\n",
    "# your function passes this test before you continue!\n",
    "X <- c(1,9,11)\n",
    "h_like_test = h %>% \n",
    "    mutate(weak_like   = map_dbl(h_vec, ~likelihood_fn(.x, X, \"weak\"))) %>%    \n",
    "    mutate(strong_like = map_dbl(h_vec, ~likelihood_fn(.x, X, \"strong\")))  %>% \n",
    "    arrange(desc(prior_combo)) %>%\n",
    "    pull(strong_like)  %>% \n",
    "   first()\n",
    "\n",
    "expect_equal(h_like_test, 0.0005787037, tolerance = 0.00001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a80f1f50e0d20802",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Computing the posterior $p(h|X) \\propto p(X|h)p(h)$\n",
    "\n",
    "We can now combine the prior and likelihood to compute the posterior distribution. The prior $p(h)$  and likelihood $p(X|h)$ will both correspond to columns in our table, and if we were mechanically following the tabular approach we could also add columns for the observation set $X$, the joint distribution $p(X,h)$, and the marginal distribution $p(X)$. The posterior could then be computed by dividing the joint distribution column $p(X,h)$ by the marginal distribution column $p(X)$.\n",
    "\n",
    "A quicker way to compute the posterior, however, is to multiply the prior and likelihood columns elementwise (which produces the joint distribution $p(X,h)$) and to normalize the resulting column so that it sums to 1. This approach means that we don't actually have to compute the marginal distribution $p(X)$ directly --- instead we exploit the fact that the posterior $p(h|X)$ is a probability distribution and must sum to 1 over the hypothesis space.\n",
    "\n",
    "The code below uses `map_dbl()` to apply `likelihood_fn()` to each hypothesis in `h`. Because we've defined 4 priors and 2 likelihood functions there are 8 models in total, and we'll compute a posterior distribution for each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f756812de209ff02",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# a function that processes a set of observations $X$ by first computing likelihoods STRONG_LIKE and WEAK_LIKE \n",
    "# according to strong and weak sampling, then combining these likelihoods with the four priors. For example, \n",
    "# WEAK_MATH is the posterior distribution for the model that assumes weak sampling and uses the mathematical \n",
    "# prior, and STRONG_COMBO is the model that assumes strong sampling and uses the combination prior\n",
    "process_X <- function(h, X) {\n",
    "  h_post <- h %>% \n",
    "    mutate(weak_like   = map_dbl(h_vec, ~likelihood_fn(.x, X, \"weak\"))) %>%    # likelihood column for weak sampling\n",
    "    mutate(strong_like = map_dbl(h_vec, ~likelihood_fn(.x, X, \"strong\"))) %>%  # likelihood column for strong sampling\n",
    "    # combine prior and likelihood \n",
    "    mutate(weak_math = weak_like * prior_math, strong_math = strong_like * prior_math, \n",
    "           weak_interval = weak_like * prior_interval, strong_interval = strong_like * prior_interval, \n",
    "           weak_uniform = weak_like * prior_uniform, strong_uniform = strong_like * prior_uniform,\n",
    "           weak_combo = weak_like * prior_combo, strong_combo = strong_like * prior_combo) %>% \n",
    "    # renormalize\n",
    "    mutate(weak_math = weak_math/sum(weak_math), strong_math = strong_math/sum(strong_math),\n",
    "           weak_interval = weak_interval/sum(weak_interval), strong_interval = strong_interval/sum(strong_interval),\n",
    "           weak_uniform = weak_uniform/sum(weak_uniform), strong_uniform = strong_uniform/sum(strong_uniform),\n",
    "           weak_combo = weak_combo/sum(weak_combo), strong_combo = strong_combo/sum(strong_combo))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d38a631dc130dcfd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's now run the models. Suppose we observe that 4 and 8 belong to concept $C$ (i.e. $X = [4,8]$). We'll compute posterior distributions $p(h|X)$ for all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cced232bf10f5c64",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "h_post <- process_X(h, c(4,8))  %>% \n",
    "    arrange(desc(strong_math)) # arrange hypotheses in descending order according to the posterior for the strong_math model\n",
    "show_table(head(h_post, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b44f27fac7beea1a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Question 6 (1 point)\n",
    "\n",
    "The observation set $X = [4,8]$ is equally consistent with \"multiples of 4\" and \"even numbers\", yet after observing $X$ you should see that the `strong_math` model thinks that \"multiples of 4\" is much more likely than \"even numbers.\" Explain why the model makes this inference. **(Max Response Length: 3 sentences)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1d1681dba87fa8cb",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9b929079b1e93c64",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "To answer the next question, please use the `arrange()` function to sort `h_post` in different ways. You can use the code cell below to try out different sortings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell to inspect h_post in various ways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9793cd85406bf35a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "### Question 7 (1 point)\n",
    "\n",
    "What is the hypothesis with greatest posterior probability according to the `strong_interval` model (ie the model that assumes strong sampling and uses the interval-based prior)? Explain why the `strong_interval` model makes this inference. **(Max Response Length: 3 sentences)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-c35b8bd3427415ce",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d3a9cf571a6f6e19",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Posterior predictive distribution $P(y \\in C | X)$\n",
    "\n",
    "We've looked at the posterior distributions assigned by the models to individual hypotheses. Let's now look at how the models generalize across the full set of numbers. We'll compute and plot the posterior predictive distribution $P(y \\in C | X)$, which shows the probability that number $y$ belongs to concept $C$ after observing the examples in $X$. Because \"posterior predictive\" is a mouthful, we'll often refer to the \"predictive\" distribution for short.\n",
    "\n",
    "The predictive distribution can be computed by summing over the entire hypothesis space $\\mathcal{H}$: \\begin{align}\n",
    "P(y \\in C | X) &= \\sum_{h \\in \\mathcal{H}} P(y \\in C | h)P(h|X)  &(1)\\\\\n",
    "               &= \\sum_{h \\in \\mathcal{H_y}}P(h|X), & (2)\n",
    "\\end{align}\n",
    "where $\\mathcal{H_y}$ is the set of all hypotheses that include $y$.\n",
    "\n",
    "For example, after observing $X = [4,8]$ the `strong_math` model computes a posterior distribution over all hypotheses $h \\in \\mathcal{H}$. The probability that 12 also belongs to the concept corresponds to the sum of the posterior probabilities assigned to all hypotheses that include 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ea21a5f74b8727d3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Compute the predictive distribution across all numbers from 1 to 12\n",
    "posterior_predictive <- function(h_post) {\n",
    "    \n",
    "  # we'll compute Equation 2 using matrix multiplication.\n",
    "  # h_mat is a binary matrix where the 12 rows represent the numbers from 1 to 12, and column i indicates which\n",
    "  # numbers belong to hypothesis i\n",
    "  h_mat <- h_post %>% \n",
    "    select(V1:V12) %>% \n",
    "    as.matrix() %>% \n",
    "    t()\n",
    "  \n",
    "  # the posterior predictive distribution for each model is now computed by multiplying h_mat by the posterior\n",
    "  # distribution for that model\n",
    "  \n",
    "  p_pred <- tibble(number  = 1:12, \n",
    "                   weak_math = as.vector(h_mat %*% h_post$weak_math),\n",
    "                   strong_math = as.vector(h_mat %*% h_post$strong_math),\n",
    "                   weak_interval = as.vector(h_mat %*% h_post$weak_interval),\n",
    "                   strong_interval = as.vector(h_mat %*% h_post$strong_interval),\n",
    "                   weak_uniform = as.vector(h_mat %*% h_post$weak_uniform),\n",
    "                   strong_uniform = as.vector(h_mat %*% h_post$strong_uniform),\n",
    "                   weak_combo = as.vector(h_mat %*% h_post$weak_combo),\n",
    "                   strong_combo = as.vector(h_mat %*% h_post$strong_combo),\n",
    "                   ) \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-713e8665ec2f3940",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We'll also need a function to visualize the predictive distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1c77dc827966838c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# code to plot the posterior predictive distributions for all 8 models given the observations in X\n",
    "plot_posterior_predictive <- function(p_pred, X) {\n",
    "  p_pred_long <- p_pred %>% \n",
    "    pivot_longer(cols = c(\"weak_math\", \"strong_math\", \"weak_interval\", \"strong_interval\", \"weak_uniform\", \"strong_uniform\", \"weak_combo\", \"strong_combo\"),\n",
    "                 names_to = \"model\",\n",
    "                 values_to = \"generalization\") %>% \n",
    "    separate(model, c(\"likelihood\", \"prior\")) %>% \n",
    "    mutate(prior = factor(prior, levels = c(\"math\", \"interval\", \"uniform\", \"combo\")))\n",
    "  \n",
    "  pic <- p_pred_long %>%\n",
    "      ggplot(aes(x = number, y = generalization)) +\n",
    "      geom_col() +\n",
    "      scale_x_continuous(breaks=1:12) +  \n",
    "      facet_grid(likelihood ~prior) +\n",
    "      xlab(\"number\") +\n",
    "      ylab(paste(\"generalization after X =\", paste(X, collapse=\" \") )) +\n",
    "    theme_bw(base_size=20)\n",
    "\n",
    "  plot(pic)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-09d3507bb051241f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Finally, let's define a wrapper function called `number_game` that takes an observation list $X$ as input, computes posterior distributions for all models and then plots predictive distributions for all models. We'll try it out by plotting predictive distributions for the observation list $X =[4,8]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ec3c9c93b5626ec6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "number_game <- function(h, X) {\n",
    "  h_post <- process_X(h, X)\n",
    "  p_pred <- posterior_predictive(h_post)\n",
    "  plot_posterior_predictive(p_pred, X)\n",
    "  return(h_post)\n",
    "}\n",
    "\n",
    "h_post <- number_game(h, c(4,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9b6cc8ec176d7ae2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In the plot above, the first row shows inferences made by the four models that assume strong sampling, and the bottom row is for models that assume weak sampling. The four columns show inferences made by models that use the four different priors (mathematical, interval, uniform, and combination).\n",
    "\n",
    "Let's change the observation set $X$ and see what happens when $X = [4,7]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b07bf244ddf26661",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "h_post <- number_game(h, c(4,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e5691af172b8e64a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Question 8 (1 point)\n",
    "\n",
    "You should see that the two models which use the uniform prior (third column) are unable to generalize beyond the observed examples. These models are able to \"memorize\" the observed examples (ie they pick up on the fact that 4 and 7 belong to the concept), but they don't make any distinctions at all between numbers outside of $X$. For these two models, why is the predictive distribution completely flat across all numbers that do not belong to $X$? **(Max Response Length: 3 sentences)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f865efeb2a8fed15",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fc09cd3926199d42",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's look at a case where $X$ includes a single example only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5babfa0729a172cd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "h_post <- number_game(h, c(8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c132b915bc48939d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Question 9 (1 point)\n",
    "\n",
    "Consider the predictions of the models with the `interval` prior (second column). Why is the predictive distribution asymmetric around the observed example 8: in particular, why does the predictive distribution assign higher probability to 7 than to 9? **(Max Response Length: 3 sentences)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5b57cd23f119f4de",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2d5b0772aa2c028d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now compare the plot we just made with the plot for the case when the same positive example is observed 5 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-454cd2c8f674743e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "h_post <- number_game(h, c(8,8,8,8,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-feb0ef54677f0c86",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You should see that the predictive distributions in the top row (strong sampling) sharpen when a single positive example is repeated multiple times. For example, consider the `strong_combo` model. After observing 8 once, the model thinks that 4 is probably also a member of the concept, and the lowest prediction made for any number is just under 0.25. But after observing 8 a total of 5 times, the model is fairly sure that 8 is the *only* member of the concept -- the predictions for all other numbers are close to zero.\n",
    "\n",
    "### Question 10 (1 point)\n",
    "\n",
    "Your friend Eustace says that it doesn't make sense for a model to sharpen its predictions in this way. If $X = [8]$, you know that 8 belongs to the concept, and $X = [8,8,8,8,8]$ essentially gives you the same information --- both observation sets are really giving you one piece of information (ie that 8 belongs to the concept). Respond to Eustace and explain why it makes sense for a model to change its inferences as the same example is repeatedly observed. **(Max Response Length: 3 sentences)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-3606e77c8ae240e6",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
