{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0426322fc5632e04",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# CBS Week 9 Notebook: Reinforcement Learning\n",
    "\n",
    "### Week 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-aeeed467706f1363",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "library(tidyverse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-56031ae3fae4c0ff",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Epsilon Greedy Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-761b85f08d29d914",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "epsilon_greedy = function(arms, N, epsilon){\n",
    "    trials = NULL\n",
    "    outcomes <- rep(1, length(arms))\n",
    "    choices <- rep(2, length(arms))\n",
    "    for(i in 1:N){\n",
    "        if(runif(1) < epsilon){\n",
    "            choice <- sample(1:length(arms), 1)\n",
    "        } else {\n",
    "            valid_arms <- which(outcomes/choices == max(outcomes/choices))\n",
    "            choice <- ifelse(length(valid_arms)==1, valid_arms, sample(valid_arms, 1))\n",
    "        }\n",
    "        outcome = rbinom(1, 1, arms[choice])\n",
    "        choices[choice] <- choices[choice] + 1\n",
    "        outcomes[choice] <- outcomes[choice] + outcome\n",
    "        trials = rbind(trials,\n",
    "                       data.frame(trial=i, choice=choice, outcome=outcome, optimal=max(arms), epsilon=epsilon))\n",
    "    }\n",
    "  trials\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cfdaa9c8d5e4f237",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Win Stay, Lose Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3dd0f03ee9b43bad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "wsls <- function(arms, N){\n",
    "    outcomes <- rep(0, length(arms))\n",
    "    choices <- rep(0, length(arms))\n",
    "    \n",
    "    choice <- sample(1:length(arms), 1)\n",
    "    outcome <- rbinom(1, 1, arms[choice])\n",
    "    \n",
    "    choices[choice] <- choices[choice] + 1\n",
    "    outcomes[choice] <- outcomes[choice] + outcome\n",
    "    trials = data.frame(trial=1, choice=choice, outcome=outcome, optimal=max(arms))\n",
    "    \n",
    "    for(i in 2:N){\n",
    "        if(outcome==1){\n",
    "            outcome <- rbinom(1, 1, arms[choice])\n",
    "        } else {\n",
    "            choice <- sample(1:length(arms), 1)\n",
    "            outcome <- rbinom(1, 1, arms[choice])\n",
    "        }\n",
    "        choices[choice] <- choices[choice] + 1\n",
    "        outcomes[choice] <- outcomes[choice] + outcome\n",
    "        trials = rbind(trials,\n",
    "                       data.frame(trial=i, choice=choice, outcome=outcome, optimal=max(arms)))\n",
    "        \n",
    "    }\n",
    "    trials\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-996287fc2da37d7d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Thompson Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f43aa8c99ca1e21d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "thompson <- function(arms, N){\n",
    "    trials = NULL\n",
    "    outcomes <- rep(0, length(arms))\n",
    "    choices <- rep(0, length(arms))\n",
    "    \n",
    "    alphas <- rep(1, length(arms))\n",
    "    betas <- rep(1, length(arms))\n",
    "    for(i in 1:N){\n",
    "        thetas <- rbeta(length(arms), alphas, betas)\n",
    "        valid_arms <- which(thetas == max(thetas))\n",
    "        choice <- ifelse(length(valid_arms)==1, valid_arms, sample(valid_arms, 1))\n",
    "        outcome <- rbinom(1, 1, arms[choice])\n",
    "        if(outcome==1){\n",
    "            alphas[choice] = alphas[choice] + 1\n",
    "        } else {\n",
    "            betas[choice] = betas[choice] + 1\n",
    "        }\n",
    "        choices[choice] <- choices[choice] + 1\n",
    "        outcomes[choice] <- outcomes[choice] + outcome\n",
    "        trials = bind_rows(trials,\n",
    "                       tibble(trial=i, choice=choice, outcome=outcome, optimal=max(arms)) %>%\n",
    "                           bind_cols(as_tibble_row(setNames(alphas/(alphas+betas), paste0('B', 1:length(arms))))) %>%\n",
    "                           bind_cols(as_tibble_row(setNames(arms, paste0('A', 1:length(arms)))))\n",
    "                          )\n",
    "    }\n",
    "    trials\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-81d869c9fd689f61",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Do these strategies work equally well across MABs?\n",
    "\n",
    "So in class we discussed two potential factors that make determining the best answer hard: the magnitude of the greatest possible reward and the relative reward between the best two options.\n",
    "\n",
    "In this tutorial, we are going to see if different sampling algorithms are better suited to different MAB problems that vary along this dimension.\n",
    "\n",
    "#### Exercise 1\n",
    "\n",
    "To start, create some MABs that vary on these two dimensions. As a reminder we define MAB problems as vectors of reward probabilities. For example, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mab_0 <- c(0.5, 0.5, 0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-510db95e7b4e966c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now for each MAB, we want to determine which strategy is the best. How do we want to measure this?\n",
    "\n",
    "#### Exercise 2\n",
    "\n",
    "Let's make a function that takes as input a simulation data.frame and outputs a numeric value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "score_simulation = function(df){\n",
    "    NA # YOUR CODE HERE\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f3e386549599ba25",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now sampling is a random process so we are going to repeat our simulations multiple times and compare the average scores. Here is some code that will run the simulations $N_{sims}$ times.\n",
    "\n",
    "Pro Tip: If you want to ensure that your simulation will return the same results, you need to set the random seed using the `set.seed` function before you start your simulation code. Otherwise, your function will return different values each time it is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a83ad95996e810ab",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "run_simulations = function(mab, horizon, N_sims, epsilon){\n",
    "    scores = NULL\n",
    "    for(i in 1:N_sims){\n",
    "        scores <- bind_rows(scores,\n",
    "                            data.frame(simulation = i,\n",
    "                                       greedy = epsilon_greedy(mab, horizon, epsilon) %>% score_simulation(),\n",
    "                                       wsls = wsls(mab, horizon) %>% score_simulation(),\n",
    "                                       thompson = thompson(mab, horizon) %>% score_simulation()\n",
    "                                      )\n",
    "                           )    \n",
    "    }\n",
    "    scores\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d8387cc519381e3a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "N_sims <- 10\n",
    "horizon <- 100\n",
    "\n",
    "sims = run_simulations(mab_0, horizon, N_sims, epsilon=0.1)\n",
    "\n",
    "sims %>%\n",
    "    gather(Sampler, Score, greedy:thompson) %>%\n",
    "    ggplot(aes(Sampler, Score)) +\n",
    "    stat_summary(fun=mean, geom='bar') +\n",
    "    stat_summary(fun.data=mean_cl_boot, geom='linerange') +\n",
    "    theme_bw(base_size=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7996200feacd48b0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Exercise 3\n",
    "\n",
    "Okay, run an experiment to test which sampling algorithm is best as the absolute magnitude of the best option changes. You need to simulate different MABs and compare their scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8fc55baf53e205f8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Exercise 4\n",
    "\n",
    "Okay, now let's run an experiment to test which sampling algorithm is best as the relative magnitude of the best two option changes. Again, you need to simulate different MABs and compare their scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ef5cfbae92cd2044",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Can we formalize the Bandits as Markov Devision Processes?\n",
    "\n",
    "In class, Frank told us that all of the bandits can be formalized as Markov-Decision Processes, which makes MDPs the Framework. Once formalized as an MDP, we can use the Bellman equation to uncover the optimal policy and value function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5b96182c6e503a4f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "bellman = function(MDP, maxiters=10000, verbose=FALSE){\n",
    "        states <- MDP$states\n",
    "        actions <- MDP$actions\n",
    "        S <- length(states)\n",
    "        A <- length(actions)\n",
    "        transitions <- MDP$transitions\n",
    "        rewards <- MDP$rewards\n",
    "        discount <- MDP$discount\n",
    "    \n",
    "        V <- rep(0, S)\n",
    "        for(i in 0:maxiters){\n",
    "            if(verbose){message(i)}\n",
    "            oldV <- V\n",
    "            Q <- matrix(0, S, A)\n",
    "            for(s in states){\n",
    "                for(a in actions){\n",
    "                    Q[s, a] <- sum((transitions[, a, s]) * (rewards[, a, s] + discount*V))\n",
    "                }\n",
    "            }\n",
    "            V <- apply(Q, 1, max, na.rm=TRUE) #rowSums(Q)\n",
    "            if(!any(abs(V-oldV) > 0.00001)){\n",
    "                break\n",
    "            }\n",
    "            if(i + 1 == maxiters){\n",
    "                message('WARNING: Values did not converge')\n",
    "            }\n",
    "        }\n",
    "        return(list(policy=apply(Q, 1, which.max), value=V))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b15e9150068def19",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's do an example. \n",
    "\n",
    "- We have a contextual bandit with 3 states and 3 actions in each state.\n",
    "- If you're in $S_1$, you always move to state $S_3$.\n",
    "- If you're in $S_2$, you always move to state $S_4$.\n",
    "- If you're in $S_3$, you always move to state $S_2$.\n",
    "- If you're in $S_1$, the arm probabilities are 0.8, 0.4 and 0.1.\n",
    "- If you're in $S_2$, the arm probabilities are 0.1, 0.4 and 0.8.\n",
    "- If you're in $S_3$, the arm probabilities are 0.1, 0.8 and 0.1.\n",
    "- $S_4$ is a goal state that says the game is over. We always need a goal state when formalizing a bandit.\n",
    "- Let's say there is no discount.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-153108a4efea7615",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Transitions is a 3 dimensional array\n",
    "    # The first dimension is the end state s'\n",
    "    # The second dimension is the action taken a\n",
    "    # The third dimension is the start state s\n",
    "# Let's initialize the transistion array with 0s\n",
    "transitions = array(0, c(4, 3, 4))\n",
    "\n",
    "# If we're in S_1 we move to S_3\n",
    "transitions[,1, 1] = c(0, 0, 1, 0)\n",
    "transitions[,2, 1] = c(0, 0, 1, 0)\n",
    "transitions[,3, 1] = c(0, 0, 1, 0)\n",
    "\n",
    "# If we're in S_2 we move to S_4\n",
    "transitions[,1, 2] = c(0, 0, 0, 1)\n",
    "transitions[,2, 2] = c(0, 0, 0, 1)\n",
    "transitions[,3, 2] = c(0, 0, 0, 1)\n",
    "\n",
    "# If we're in S_3 we move to S_2\n",
    "transitions[,1, 3] = c(0, 1, 0, 0)\n",
    "transitions[,2, 3] = c(0, 1, 0, 0)\n",
    "transitions[,3, 3] = c(0, 1, 0, 0)\n",
    "\n",
    "# Rewards are also a 3 dimensional array\n",
    "    # The first dimension is the end state s'\n",
    "    # The second dimension is the action taken a\n",
    "    # The third dimension is the start state s\n",
    "# Let's initialize the reward array with 0s\n",
    "rewards = array(0, c(4, 3, 4))\n",
    "\n",
    "# If you're in S_1, the arm probabilities are 0.8, 0.4 and 0.1\n",
    "rewards[,1, 1] = 0.8\n",
    "rewards[,2, 1] = 0.4\n",
    "rewards[,3, 1] = 0.1\n",
    "\n",
    "# If you're in S_2, the arm probabilities are 0.1, 0.4 and 0.8\n",
    "rewards[,1, 2] = 0.1\n",
    "rewards[,2, 2] = 0.4\n",
    "rewards[,3, 2] = 0.8\n",
    "\n",
    "# If you're in S_3, the arm probabilities are 0.1, 0.8 and 0.1\n",
    "rewards[,1, 3] = 0.1\n",
    "rewards[,2, 3] = 0.8\n",
    "rewards[,3, 3] = 0.1\n",
    "\n",
    "MDP = list(states= c(1, 2, 3, 4),\n",
    "     actions = c(1, 2, 3),\n",
    "     rewards = rewards,\n",
    "     transitions = transitions,\n",
    "     discount=1.0)\n",
    "\n",
    "bellman(MDP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3618c0603c8a7e81",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "So according to the Bellman equation, the optimal policy is to choose the first arm when in $S_1$; the third arm in $S_2$; and the second arm in $S_3$.\n",
    "\n",
    "Further, if we start in $S_1$, we can expect to win 2.4 times using the optimal policy---i.e., 0.8 in the first state, 0.8 in the third state and 0.8 in the second state. If we start in $S_2$, we can expect to win 0.8 times. If we start to win in $S_3$, we can expect to win 1.6 times. \n",
    "\n",
    "When interpreting, we ignore the goal state $S_4$ as there are no actions to take and no reward to win."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7f462ffef9dc623e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Last Exercise\n",
    "\n",
    "It's your turn.\n",
    "\n",
    "Hyssop's bored of information theory and wants to run a contextual bandit task. They're keeping it simple. Participants must choose whether to explore the dungeons or the towers of a castle. Trials occur either at night or during the day. At night, the dungeons are 20% likely to have loot and the towers are 10% likely to have loot. During the day, the towers are 30% likely to have loot and the dungeons are 15% likely to have loot. Raiding during the day normally takes all day but 10% of the time, two raids can happen in the same day. Raiding at night is fast, so only 10% of the time will a raid take all night.Participants get to storm 5 castles before the experiment ends.\n",
    "\n",
    "He wants to use a discount parameter of $\\gamma=0.8$.\n",
    "\n",
    "Hyssop wants to see what the optimal policy is for his experiment. Specifically, they want to know:\n",
    "\n",
    "- Under the optimal policy, what should a participant do if it's nightime on the 3rd trial?\n",
    "- If a participant knows the optimal policy, can they score more if their first trial is a day trial or a night trial?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "states = c()\n",
    "actions = c()\n",
    "\n",
    "S = length(states)\n",
    "A = length(actions)\n",
    "\n",
    "transitions = array(0, c(S, A, S))\n",
    "\n",
    "reward = array(0, c(S, A, S))\n",
    "\n",
    "\n",
    "MDP = list(states=states, \n",
    "           actions=actions,\n",
    "           transitions=transitions,\n",
    "           rewards=reward,\n",
    "           discount=0.8)\n",
    "\n",
    "bellman(MDP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
