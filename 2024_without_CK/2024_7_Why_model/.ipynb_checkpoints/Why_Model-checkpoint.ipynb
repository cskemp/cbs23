{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9a3edcde54ff1837",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Why (Bayesian) Model?\n",
    "\n",
    "In this week's lecture, we defined computational modelling and the process of building a computational model. Hopefully, you all realised that there was no actual coding required to implement the skills and frameworks we covered in class. So in this week's tutorial, I want to get you acquainted to how I write code in R, which is more `tidyverse` than `base` (what HRMI relies on).\n",
    "\n",
    "The code (and solutions) given in this notebook will use functions from the tidyverse, including `mutate()`, `filter()`, `pull()`, `gather()` and the pipe operator `%>%`. If you're not familiar with the tidyverse or would like to refresh your memory, please take a look at [R for psychological science]( https://psyr.djnavarro.net/index.html ) by Danielle Navarro. The following sections discuss core elements of the tidyverse:\n",
    "* https://psyr.djnavarro.net/prelude-to-data.html\n",
    "* https://psyr.djnavarro.net/describing-data.html\n",
    "* https://psyr.djnavarro.net/manipulating-data.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f00bd44f3f53aea5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "options(jupyter.plot_scale=1) # I like big plots\n",
    "\n",
    "library(tidyverse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-77b1e80cea531849",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In this tutorial, we are going to illustrate a few of the concepts from lecture working through an example from word learning. In language acquisition, researchers have spent a long time trying to predict what makes some words learned earlier than other words [(see the Wordbank book)](https://wordbank-book.stanford.edu/). In these analyses, researchers ask parents to complete a survey of some 600ish words that their child may or may not understand or produce. Researchers then take a predictor variable, like word frequency or word concreteness, and use some kind of computational model to predict the age of acquisition. \n",
    "\n",
    "For many years, researchers used logistic regression as an off-the-shelf linking hypothesis. I don't expect you to know all the details of logistic regression. I'm going to try and give you enough understanding to follow along. I have designed the exercises in this notebook to be achievable without some of the advanced knowledge I'm going to reference. If you can, you should work with a group through this notebook. \n",
    "\n",
    "In logistic regression, we want to predict a binary variable $y$ (Yes they can produce a word / No they can't) as a function of some predictor variable $x$ (age).\n",
    "$$ y = f(x) .$$\n",
    "We call it logistic regression because we use the logistic function:\n",
    "$$ f(x) = \\frac{1}{1 + e^{-(\\beta_1 x + \\beta_0)}},$$\n",
    "where $\\beta_1$ is a scaling parameter and $\\beta_0$ is a base rate---i..e, how likely will kids know this word independent of age.\n",
    "\n",
    "Let's show you logistic regression in action. Let's use [Wordbank](https://wordbank-book.stanford.edu/), a large collection of child age of acquisition data, to predict when some Australian children learn to say \"kangaroo.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ed6d897f3a131288",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# I've already downloaded the Australian data from Wordbank so let's load it in\n",
    "wb = read.csv('wordbank_instrument_data.csv')\n",
    "\n",
    "# And today I only care about kangaroo, which is item_30\n",
    "d = wb %>% filter(item_id == 'item_30')\n",
    "\n",
    "head(d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-12b7669f803303d1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "There's a lot of columns in this data.frame including when I downloaded the dataset `downloaded`, word properties like  `category` and child identifiers `child_id`. Today, we are going to focus on `age` and whether or not the parent reported their child could say \"kangaroo\" `value`. `value` can take three possible values: blank, 'produces' or NA. NA's happen when a parent accidentally skips a question on the survey. As a first step, let's filter out the missing data. Then let's create a binary variable for whether a child produces \"kangaroo\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-763c214b01d8217e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "d = d %>%\n",
    "    filter(!is.na(value)) %>% # remove the missing data; sometimes parents skip a page \\_o_/\n",
    "    mutate(produces = ifelse(value == '', 0, 1)) # let's recode the data into 1=produces and 0=can't produce\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5c00eaee205844c4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now I don't know about y'all but when I get data, the first thing I want to do is plot it. `ggplot` is an excellent R plotting tool that's build on a visualization called the grammar of graphics (hence the gg). We build plots layer by layer giving us full control over the graph. I'll annotate the code the first time I do something so you can follow along.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-40fbdd6c87240e22",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "d %>%\n",
    "    ggplot(aes(x=age, y=produces)) + # I'm going to make a plot with age on the x-axis and produces on the y-axis\n",
    "    stat_summary(fun=mean, geom='line') + # Let's take the mean of produces at each age and connect them with a line\n",
    "    stat_summary(fun=mean, geom='point') + # Now let's put a point on the means at each age\n",
    "    stat_summary(fun.data=mean_cl_boot, geom='linerange') + # Let's add some bootstrapped confidence intervals\n",
    "    theme_bw(base_size=20) + # I like clean themes \\_o_/ and let's make the font size big (20)\n",
    "    xlab('Age (months)') + # Add an x-axis label\n",
    "    ylab('Proportion kids producing') + # Now a y-axis label\n",
    "    ggtitle('kangaroo') + # Let's give it a title\n",
    "    coord_cartesian(ylim=c(0, 1)) + # Now produces is binary variable so let's set the y-axis to span [0,1]\n",
    "    theme(plot.title = element_text(hjust = 0.5)) # Let's center the title\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-187861beb8a70aa4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now that's pretty :)\n",
    "\n",
    "**Back to logistic regression.** So now we can use logistic regression to predict this acquisition curve as a function of age. We have a few steps to this:\n",
    "\n",
    "1. Aggregate the data by age.\n",
    "2. Fit a logistic regression model using `glm`\n",
    "3. Grab the model predictions\n",
    "4. Plot\n",
    "\n",
    "Let's do them all in the next chunk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6608eeedd3e40431",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Step 1\n",
    "d.agg = d %>%\n",
    "    group_by(age, uni_lemma) %>%  # For each age\n",
    "    summarise(successes = sum(produces),  # Let's get the number of kids who can successfully say kangaroo\n",
    "           failures = n() - successes) %>% # and the number of kids who fail to say kangaroo\n",
    "    ungroup() # and we ungroup to keep good habits\n",
    "\n",
    "\n",
    "# Step 2\n",
    "# Logistic regression is 1 line in R\n",
    "    # The first argument says predict (success, failure) using age\n",
    "    # The family argument says use the logistic function as a linking hypothesis\n",
    "g.Logit  <- glm(cbind(successes, failures) ~ age, family=binomial(link=\"logit\"),  data=d.agg)\n",
    "\n",
    "\n",
    "# Step 3\n",
    "# Let's see the model response predictions on the aggregated dataset\n",
    "d.agg = d.agg %>%\n",
    "    mutate(Logit=predict(g.Logit, newdata=d.agg, type=\"response\"))\n",
    "\n",
    "# Step 4\n",
    "# Make a pretty plot\n",
    "d %>%\n",
    "    ggplot(aes(x=age, y=produces)) +\n",
    "    # Instead of a line for the data, let's plot the predictions as a line\n",
    "        # So we want to use a different data for this layer, \n",
    "        # We'll specify the axes\n",
    "        # And for good measure, let's color this line blue \n",
    "    geom_line(data=d.agg, aes(x=age, y=Logit), color='blue') + \n",
    "    # Now we just back to using the original dataset d\n",
    "    stat_summary(fun=mean, geom='point') +\n",
    "    stat_summary(fun.data=mean_cl_boot, geom='linerange') +\n",
    "    theme_bw(base_size=20) +\n",
    "    xlab('Age (months)') +\n",
    "    ylab('Proportion kids producing') +\n",
    "    ggtitle('kangaroo') +\n",
    "    coord_cartesian(ylim=c(0, 1)) +\n",
    "    theme(plot.title = element_text(hjust = 0.5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c60eeaa4e0a03527",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You can't argue with how pretty that plot is :)\n",
    "\n",
    "But is a logistic function a good linking hypothesis? Put simply, this linking hypothesis says:\n",
    "<center>\n",
    "    <br>\n",
    "    Word learning is a function of a child's age.\n",
    "</center>\n",
    "\n",
    "### Exercise 1: What's in a linking hypothesis?\n",
    "\n",
    "Let's try and critique this linking hypothesis using what we've learning in class. I'll outline some questions that you might consider. You certainly don't need to answer all of them. I want you to form your own opinion on whether this is a *reasonable* linking hypothesis and be prepared to argue for your opinion. Use the next cell to jot down your thoughts, make a pro/con list or whatever helps you organize your thoughts.\n",
    "\n",
    "- Framework: What are the first principles behind this hypothesis?\n",
    "- Theory: What kinds of theories might this linking hypothesis (dis-)agree with?\n",
    "- Specification: Are there any concerns about this specification?\n",
    "- Implementation: Did I do anything unreasonable in the implementation?\n",
    "- Hypothesis: Does the linking hypothesis allow us to make useful hypotheses about word learning?\n",
    "- Data: Are the data coherent with the linking hypothesis?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-60533ea47972e3b4",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "\n",
    "Your thoughts here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-75db8ace6cdf30c2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "# Don't read further until you've completed Exercise 1\n",
    "\n",
    "Well, I have some opinions here and I've even [spilt ink over it](https://dx.doi.org/10.1162/opmi_a_00006).\n",
    "\n",
    "As I see it, it is a truth universally acknowledged that children need to observe linguistic input before they can learn the meaning of a word. But how much data do children use to learn words? Some studies have said that children can learn from a single learning instance (Carey & Bartlett, 1978); whereas, other studies suggest children need tens, hundreds or even thousands of instances before they can learn a word (Smith & Yu, 2008). \n",
    "\n",
    "But also shouldn't the quality of a learning instance count? If I just sit there and say \"ball\" over and over and over again is that three instances or just one? Hanging out with my niece, it certainly feels like it doesn't count at all! Does it count if she overheard me say \"cornbread\" while she's playing with a toy car? So maybe we don't care about the raw frequency of explosure at all. Instead, we want to know the rate of \"effective\" learning instances.\n",
    "\n",
    "In my younger and more impressionable years, I came across an analysis by Hidaka (2013) that uses a two-parameter linking hypothesis to account for these critiques. Using a gamma linking hypothesis, we model word learning as how long it takes for a learner to observe $k$ effective learning instances when effective learning instances come at a constant rate $\\lambda$ per month.\n",
    "\n",
    "Let's fit one of these gamma models on our kangaroo data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f072fb172cb2aa2e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Here's the gamma model, let's not worry about it\n",
    "gammaModel = function(params){\n",
    "    k = params[1]\n",
    "    lambda = params[2]\n",
    "    -1*sum(dbinom(d.agg$successes, d.agg$successes + d.agg$failures, p=pgamma(d.agg$age, k, lambda), log=TRUE))\n",
    "}\n",
    "\n",
    "# Here's the \"regression\" function (again let's not worry about it)\n",
    "o = optim(c(10, 1/2), gammaModel, method = \"L-BFGS-B\", lower = c(0, 0))\n",
    "\n",
    "# Let's grab our predictions\n",
    "d.agg = d.agg %>%\n",
    "    mutate(Gamma=pgamma(age, o$par[1], o$par[2]))\n",
    "\n",
    "# Finally the plot thickens!\n",
    "d %>%\n",
    "    ggplot(aes(x=age, y=produces)) +\n",
    "    # Now that we have two models, we have to do a bit of data wrangling\n",
    "    # Right now, we have one column for each of our model predictions\n",
    "    # We want one column with all of the model predictions: prediction\n",
    "    # And another column telling us which model each prediction comes from\n",
    "    # This can be done using `gather` in the tidyverse\n",
    "    geom_line(data=d.agg %>% gather(model, prediction, Logit:Gamma), \n",
    "              aes(x=age, y=prediction, color=model, linetype=model), size=1) +\n",
    "    stat_summary(fun=mean, geom='point') +\n",
    "    stat_summary(fun.data=mean_cl_boot, geom='linerange') +\n",
    "    theme_bw(base_size=20) +\n",
    "    xlab('Age (months)') +\n",
    "    ylab('Proportion kids producing') +\n",
    "    ggtitle('kangaroo') +\n",
    "    coord_cartesian(ylim=c(0, 1)) +\n",
    "    theme(plot.title = element_text(hjust = 0.5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-307d405a3688ee2e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### B-E-A-UTIFUL\n",
    "\n",
    "But this isn't the only horse in the race! Other's have suggested linking hypotheses for word learning. For exampe, a famous linking hypothesis was proposed by Bob McMurray (2007) to account for the *vOcAbUlArY eXpLoSiOn*!\n",
    "\n",
    "On average, children start to acquire their first words between 12-18 months. While the first few words take a long time to learn, the next 50 words are aquired rapidly. This observation is lovingly referred to as the vocabulary explosion. For a long time, the vocabulary explosion puzzled researchers, who proposed complicated models of how children's ability to process language must rapidly change. \n",
    "\n",
    "In 2007, Bob McMurray proposed a simple model to account for the explosion. He started with the question: <b>what would happen if a child accumulates information about each word at the same rate in parallel and some words were more difficult than others</b>. In Figure panel A, we see that he places a normal (Gaussian) distribution over how difficult it is to learn words. Few words are really easy to learn (requiring few timesteps) and few words are really difficult to learn (requiring many timesteps). In panel B, we see the results of this simulation for acquiring 10,000 words. In panel C, Bob repeats the simulation; however, this time he defines difficulty in terms of a word's frequency. More frequent words are easier to acquire and there are only a few frequent words in language. Bob concludes that <b>\"Acceleration is guaranteed in any system in which (i) words are acquired in parallel... and (ii) the difficulty of learning words is distributed such that there are few words that can be acquired quickly and a greater number that will take longer.\"</b>\n",
    "\n",
    "![McMurray](images/mcmurray.png)\n",
    "\n",
    "\n",
    "### Exercise Two: Core vs Auxilliary Assumptions\n",
    "\n",
    "In this description, there are several components of the model specification. Which of the assumptions are core and which are auxiliary? Try it yourself first and then compare with a group.\n",
    "\n",
    "1. Difficulty is Gaussian distributed.\n",
    "2. Difficulty is distributed such that there are a few easy words and more hard words.\n",
    "3. Information about words is acquired at the same rate.\n",
    "4. Information about all words are acquired simultaneously.\n",
    "5. Difficulty is distributed according to frequency. \n",
    "6. Information about 10000 words are acquired simultaneously.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-0744b9a852068082",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "\n",
    "You can jot down your answers for safe keeping here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fd97035916f79eae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Don't read further until you've completed Exercise 2\n",
    "\n",
    "Bob's model is equivalent to a probit regression (which you definitely don't need to know). It's really elegant (and it's only one line of R code) so let's add it to our plot!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b276a2b0631652fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "g.Probit <- glm(cbind(successes, failures) ~ age, family=binomial(link=\"probit\"), data=d.agg)\n",
    "\n",
    "# Make some predictions\n",
    "d.agg = d.agg %>%\n",
    "    mutate(Probit=predict(g.Probit, newdata=d.agg, type=\"response\"))\n",
    "\n",
    "# PLOT ALL THE THINGS!!!\n",
    "d %>%\n",
    "    ggplot(aes(x=age, y=produces)) +\n",
    "    geom_line(data=d.agg %>% gather(model, prediction, Logit:Probit), \n",
    "              aes(x=age, y=prediction, color=model, linetype=model), size=1) +\n",
    "    stat_summary(fun=mean, geom='point') +\n",
    "    stat_summary(fun.data=mean_cl_boot, geom='linerange') +\n",
    "    theme_bw(base_size=20) +\n",
    "    xlab('Age (months)') +\n",
    "    ylab('Proportion kids producing') +\n",
    "    ggtitle('kangaroo') +\n",
    "    coord_cartesian(ylim=c(0, 1)) +\n",
    "    theme(plot.title = element_text(hjust = 0.5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1995d801c6c77d4f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Ok so this is pretty, but like boring. All the models predict the same thing :/\n",
    "\n",
    "From a high level evaluation standpoint, we can make arguments for why one might be a better explanation/description, more coherent, valid or tractable than the others. But at least two of these linking hypotheses seem *reasonable*$^\\dagger$. Actually, this is a nice demo for why we should be using multiple methods of evaluation! \n",
    "\n",
    "### Exercise Three: Weak or Strong Generalization\n",
    "\n",
    "In the next two chunks, I've provided code for doing held-out prediction. All you have to do is provide the filter condition to define the training data. I've flagged it with SCARY ALL CAPS COMMENTS. Use the first cell to illustrate weak generalization (where the held-out data is within the training range) and the second cell to illustrate strong generalization (where the held-out data is out of the training range). Then summarise your findings.\n",
    "\n",
    "$^\\dagger$ At least to me and I am the veritable and just arbitrator of reason (at least for this tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weak Generalization\n",
    "\n",
    "train = d %>%\n",
    "    # ADD YOUR FILTER CONDITION HERE \n",
    "    # filter() %>%\n",
    "    group_by(age, uni_lemma) %>%\n",
    "    summarise(successes = sum(produces),\n",
    "           failures = n() - successes) %>%\n",
    "    ungroup()\n",
    "\n",
    "test = d %>%\n",
    "    group_by(age, uni_lemma) %>%\n",
    "    summarise(successes = sum(produces),\n",
    "           failures = n() - successes) %>%\n",
    "    ungroup()\n",
    "\n",
    "\n",
    "g.Logit  <- glm(cbind(successes, failures) ~ age, family=binomial(link=\"logit\"),  data=train)\n",
    "g.Probit <- glm(cbind(successes, failures) ~ age, family=binomial(link=\"probit\"), data=train)\n",
    "\n",
    "gammaModel = function(params){\n",
    "    k = params[1]\n",
    "    lambda = params[2]\n",
    "    -1*sum(dbinom(train$successes, train$successes + train$failures, p=pgamma(train$age, k, lambda), log=TRUE))\n",
    "}\n",
    "\n",
    "o = optim(c(21, 0.12), gammaModel, method = \"L-BFGS-B\", lower = c(0, 0), upper = c(Inf, Inf))\n",
    "\n",
    "pred = test %>%\n",
    "    mutate(Logit=predict(g.Logit, newdata=test, type=\"response\"),\n",
    "           Probit=predict(g.Probit, newdata=test, type=\"response\"),\n",
    "           Gamma=pgamma(age, o$par[1], o$par[2]))\n",
    "\n",
    "d %>%\n",
    "    # ADD YOUR FILTER CONDITION IN THE ifelse BELOW\n",
    "    mutate(splt = ifelse(#FILTER CONDITION HERE#, 'Train', 'Test')) %>% \n",
    "    ggplot(aes(x=age, y=produces)) +\n",
    "    geom_line(data=pred %>% gather(model, prediction, Logit:Gamma), \n",
    "              aes(x=age, y=prediction, color=model, linetype=model), size=1) +\n",
    "    stat_summary(fun=mean, geom='point', aes(shape=splt), size=2) +\n",
    "    stat_summary(fun.data=mean_cl_boot, geom='linerange') +\n",
    "    theme_bw(base_size=20) +\n",
    "    xlab('Age (months)') +\n",
    "    ylab('Proportion kids producing') +\n",
    "    ggtitle('kangaroo') +\n",
    "    coord_cartesian(ylim=c(0, 1)) +\n",
    "    theme(plot.title = element_text(hjust = 0.5))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strong Generalization\n",
    "\n",
    "train = d %>%\n",
    "    # ADD YOUR FILTER CONDITION HERE \n",
    "    # filter() %>%\n",
    "    group_by(age, uni_lemma) %>%\n",
    "    summarise(successes = sum(produces),\n",
    "           failures = n() - successes) %>%\n",
    "    ungroup()\n",
    "\n",
    "test = d %>%\n",
    "    group_by(age, uni_lemma) %>%\n",
    "    summarise(successes = sum(produces),\n",
    "           failures = n() - successes) %>%\n",
    "    ungroup()\n",
    "\n",
    "\n",
    "g.Logit  <- glm(cbind(successes, failures) ~ age, family=binomial(link=\"logit\"),  data=train)\n",
    "g.Probit <- glm(cbind(successes, failures) ~ age, family=binomial(link=\"probit\"), data=train)\n",
    "\n",
    "gammaModel = function(params){\n",
    "    k = params[1]\n",
    "    lambda = params[2]\n",
    "    -1*sum(dbinom(train$successes, train$successes + train$failures, p=pgamma(train$age, k, lambda), log=TRUE))\n",
    "}\n",
    "\n",
    "o = optim(c(21, 0.12), gammaModel, method = \"L-BFGS-B\", lower = c(0, 0), upper = c(Inf, Inf))\n",
    "\n",
    "pred = test %>%\n",
    "    mutate(Logit=predict(g.Logit, newdata=test, type=\"response\"),\n",
    "           Probit=predict(g.Probit, newdata=test, type=\"response\"),\n",
    "           Gamma=pgamma(age, o$par[1], o$par[2]))\n",
    "\n",
    "d %>%\n",
    "    # ADD YOUR FILTER CONDITION IN THE ifelse BELOW\n",
    "    mutate(splt = ifelse(#FILTER CONDITION HERE#, 'Train', 'Test')) %>% \n",
    "    ggplot(aes(x=age, y=produces)) +\n",
    "    geom_line(data=pred %>% gather(model, prediction, Logit:Gamma), \n",
    "              aes(x=age, y=prediction, color=model, linetype=model), size=1) +\n",
    "    stat_summary(fun=mean, geom='point', aes(shape=splt), size=2) +\n",
    "    stat_summary(fun.data=mean_cl_boot, geom='linerange') +\n",
    "    theme_bw(base_size=20) +\n",
    "    xlab('Age (months)') +\n",
    "    ylab('Proportion kids producing') +\n",
    "    ggtitle('kangaroo') +\n",
    "    coord_cartesian(ylim=c(0, 1)) +\n",
    "    theme(plot.title = element_text(hjust = 0.5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-9edaa528642565aa",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "\n",
    "Summarise your results here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ced9481c65a1e110",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "Ok Okay OK I know you're all tired and done with this. Hopefully, you have a better idea of how to use some of the tools that we talked about in class. I promise we'll get back to real coding exercises next time. \n",
    "\n",
    "One parting exercise, just to make sure you understand Marr's levels.\n",
    "\n",
    "### Exercise Four: Marr's Levels\n",
    "\n",
    "In the following images, it is your job to assign each sub-image to Marr's levels. Try it on your own and discuss with a group. Put your answers and any comments into the following cell.\n",
    "\n",
    "Image one:\n",
    "![knitting](images/marrQ.png)\n",
    "\n",
    "Image two:\n",
    "![pdp](images/marrQ2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-eb360da14d531c48",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "\n",
    "Record your answers here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ba7a85de75469578",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## References\n",
    "\n",
    "Carey, S., & Bartlett, E. (1978). Acquiring a single new word. *Papers and Reports on Child Language Development, 15,* 17–29.\n",
    "\n",
    "Frank, M. C., Braginsky, M., Yurovsky, D., and Marchman, V. A. (2021). Variability and Consistency in Early Language Learning: The Wordbank Project. Cambridge, MA: MIT Press.\n",
    "\n",
    "Hidaka, S. (2013). A computational model associating learning process, word attributes, and age of acquisition. *PLOS one, 8*(11), e76242.\n",
    "\n",
    "McMurray, B. (2007). Defusing the childhood vocabulary explosion. *Science, 317*(5838), 631-631.\n",
    "\n",
    "Mollica, F., & Piantadosi, S. T. (2017). How data drive early word learning: A cross-linguistic waiting time analysis. *Open Mind, 1*(2), 67-77.\n",
    "\n",
    "Smith, L., & Yu, C. (2008). Infants rapidly learn word-referentmappings via cross-situational statistics. *Cognition, 106*(3),1558–1568."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
