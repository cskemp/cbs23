{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-84b67070ad409557",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# CBS Week 1 Exercise: Jupyter Notebooks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-17a4fb7156cee4c7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "suppressPackageStartupMessages({\n",
    "    library(testthat)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-30c36b6c00235753",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "A discussion thread is available for each week under the Discussions tab on the learning management system. If you have a question, please check the discussion to see if your question has already been addressed. If it hasn't been addressed, please post your question to the thread. Almost all class-related questions should be posted to the discussion board instead of sent to the instructors directly --- that way everyone in the class can see the responses. If you see a question on the Discussion board that you're able to answer, please jump in and do so.\n",
    "\n",
    "## Introduction to Jupyter Notebooks\n",
    "\n",
    "Future assignments and tutorials will take the form of Jupyter notebooks like this one. Typically you'll be asked to edit some of the code chunks and to add text in response to short answer questions. This introductory notebook will be graded automatically but the grade will not count. Completing and submitting the notebook, however, will get you ready for next week's tutorial and the notebooks to come.\n",
    "\n",
    "Notebooks may include questions that ask for a free-text response. For example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cdb9de9629872aeb",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Exercise 1 (0 points)\n",
    "\n",
    "Name one way in which human brains are superior to the best current digital electronic computers. Provide your answer in the cell immediately below this one.\n",
    "\n",
    "=== BEGIN MARK SCHEME === \n",
    "\n",
    "This question is a demo only -- but if it were graded you'd get full credit for any reasonable answer: e.g. human brains use less power, are more fault-tolerant, etc\n",
    "    \n",
    "=== END MARK SCHEME ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-95d577de8bb7e76c",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3a0a6db78d2674fd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Notebooks may also include questions that ask you to write or edit some code. For example:\n",
    "\n",
    "### Exercise 2 (1 point)\n",
    "\n",
    "Complete the definition of a function `add_pair` that returns the sum of two numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-32136b2aad56eb90",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "add_pair <- function(a,b){\n",
    "    ### BEGIN SOLUTION\n",
    "    return (a+b)\n",
    "    ### END SOLUTION\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a656baf4c1019133",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Some coding questions will be automatically graded. For example, the automatic grader might check that your `add_pair` function produces the right answers on a couple of examples. The tests here are formulated using the `testthat` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-66187dd404b05458",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "expect_that(add_pair(1,4), equals(5))\n",
    "expect_that(add_pair(-100,-8), equals(-108))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b01c5d68808699ed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Often the tests will be hidden. For example, here's a completely unfair question that we would never ask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3ec40589600fd824",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 3 (1 point)\n",
    "I'm thinking of an integer `x` between 1 and 1,000,000. What is `x`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-59a3e4a10fe8f7fa",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "x <-\n",
    "    ### BEGIN SOLUTION\n",
    "    520981\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-aa76c5832e2fbae1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Here's one test that your solution should pass. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-9fffd71e768b9864",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "expect_that(x>0, equals(TRUE))\n",
    "### BEGIN HIDDEN TESTS\n",
    "expect_that(x, equals(520981))\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-aa76c5832e2fbae2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "But there's also a hidden test that you're not seeing which checks whether your guess matches the true value of `x`. Unless you're very lucky your solution will fail this test (but don't worry -- this notebook doesn't count for anything!)\n",
    "\n",
    "After you've finished working through the notebook, try \"validating\" the notebook by pressing the Validate button in the toolbar above. The validation process checks that your code passes all of the *visible* tests but doesn't consider the hidden tests. So just because the notebook validates doesn't mean that all of your answers are correct. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": "",
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
