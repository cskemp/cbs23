{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf3a73e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b0f8b7d00bf547fb20d8101f2965a928",
     "grade": false,
     "grade_id": "cell-d162386d3bb121ad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "library(tokenizers)\n",
    "library(stringr)\n",
    "library(neuralnet)\n",
    "library(testthat)\n",
    "options(warn=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891ff95d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6c3068826be0be5b3bc4cb14975e1601",
     "grade": false,
     "grade_id": "cell-067b996f18d00594",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## The Word2Vec Assignment\n",
    "\n",
    "In the assignment for the semantic models chapter, we are going to apply word2vec to a somewhat larger corpus of text drawn from the children's book \"Green eggs and ham\" by Dr. Zeus. Again we use a children's book, because the text is repetitive and so the model has common contexts to work with but doesn't take too long to train.\n",
    "\n",
    "Note I have removed quotation marks because they break the rules of field names in R and I have changed question marks to the token QUESTION, for the same reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc6d05f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "64b2f3f410e8effce60a8283f4aac013",
     "grade": false,
     "grade_id": "cell-b304fe3bd5366d4b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "text = \"DO WOULD YOU LIKE GREEN EGGS AND HAM QUESTION\n",
    "\n",
    "I DO NOT LIKE THEM SAM I AM .\n",
    "I DO NOT LIKE GREEN EGGS AND HAM .\n",
    "\n",
    "WOULD YOU LIKE THEM HERE OR THERE QUESTION\n",
    "\n",
    "I WOULD NOT LIKE THEM HERE OR THERE .\n",
    "I WOULD NOT LIKE THEM ANYWHERE .\n",
    "I DO NOT LIKE GREEN EGGS AND HAM .\n",
    "I DO NOT LIKE THEM SAM I AM .\n",
    "\n",
    "WOULD YOU LIKE THEM IN A HOUSE QUESTION\n",
    "WOULD YOU LIKE THEN WITH A MOUSE QUESTION\n",
    "\n",
    "I DO NOT LIKE THEM IN A HOUSE .\n",
    "I DO NOT LIKE THEM WITH A MOUSE .\n",
    "I DO NOT LIKE THEM HERE OR THERE .\n",
    "I DO NOT LIKE THEM ANYWHERE .\n",
    "I DO NOT LIKE GREEN EGGS AND HAM .\n",
    "I DO NOT LIKE THEM SAM I AM .\n",
    "\n",
    "WOULD YOU EAT THEM IN A BOX QUESTION\n",
    "WOULD YOU EAT THEM WITH A FOX QUESTION\n",
    "\n",
    "NOT IN A BOX . NOT WITH A FOX .\n",
    "NOT IN A HOUSE . NOT WITH A MOUSE .\n",
    "I WOULD NOT EAT THEM HERE OR THERE .\n",
    "I WOULD NOT EAT THEM ANYWHERE .\n",
    "I WOULD NOT EAT GREEN EGGS AND HAM .\n",
    "I DO NOT LIKE THEM SAM I AM .\n",
    "\n",
    "WOULD YOU QUESTION COULD YOU QUESTION IN A CAR QUESTION\n",
    "EAT THEM EAT THEM HERE THEY ARE .\n",
    "\n",
    "I WOULD NOT COULD NOT IN A CAR .\n",
    "\n",
    "YOU MAY LIKE THEM . YOU WILL SEE .\n",
    "YOU MAY LIKE THEM IN A TREE\n",
    "\n",
    "I WOULD NOT COULD NOT IN A TREE .\n",
    "NOT IN A CAR YOU LET ME BE .\n",
    "I DO NOT LIKE THEM IN A BOX .\n",
    "I DO NOT LIKE THEM WITH A FOX .\n",
    "I DO NOT LIKE THEM IN A HOUSE .\n",
    "I DO NOT LIKE THEM WITH A MOUSE .\n",
    "I DO NOT LIKE THEM HERE OR THERE .\n",
    "I DO NOT LIKE THEM ANYWHERE .\n",
    "I DO NOT LIKE GREEN EGGS AND HAM .\n",
    "I DO NOT LIKE THEM SAM I AM .\"\n",
    "\n",
    "corpus = tokenize_ptb(tolower(text))\n",
    "corpus\n",
    "T = length(corpus[[1]])\n",
    "print(paste0(\"corpus size = \", T))\n",
    "for (i in 1:T){\n",
    "    if (corpus[[1]][i] == \"1\"){\n",
    "        print (i)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd499df0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "64d2740e6ca0966eccd71b9ed5cbd3d7",
     "grade": false,
     "grade_id": "cell-ecd24e2a7086d96f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As in the tutorial, we create the vocabulary (vocab) and the index (I) that maps words into unique dimensions in preparation for creating the training patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a34f51",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0ec32a73a1c8e60e10c2a1dda09c2d58",
     "grade": false,
     "grade_id": "cell-f573838b1f71315e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "vocab = unique(str_sort(corpus[[1]]))\n",
    "V = length(vocab)\n",
    "I = 1:V\n",
    "names(I) = vocab\n",
    "\n",
    "print(paste0(\"vocabulary size = \", V))\n",
    "vocab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3a7f3e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "be771da978b9db7e2c64b75d7e106039",
     "grade": false,
     "grade_id": "cell-d1343d9a1f107c4d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we can create the dataframe that contains our patterns. Note that we set the context size (C) to 1 in this case. Later, we will explore how changing the context size changes the output of the model. You will need to adjust the size in this cell and regenerate the dataframe.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50fe6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "contextvars = NULL\n",
    "targetvars = NULL\n",
    "for (i in 1:V){\n",
    "    contextvars = c(contextvars, paste0(vocab[i], \"C\"))\n",
    "    }\n",
    "for (i in 1:V){\n",
    "    targetvars = c(targetvars, paste0(vocab[i], \"T\"))\n",
    "    }\n",
    "\n",
    "C = 1 # context size\n",
    "df = data.frame(matrix(0, T, V*2))\n",
    "\n",
    "colnames(df) = c(contextvars, targetvars)\n",
    "\n",
    "for (i in 1:T){\n",
    "    target = corpus[[1]][i]\n",
    "    df[i, I[[target]]+V] = 1\n",
    "    for (j in (i-C):(i+C)){\n",
    "        if (j >= 1 && j <= T && i != j){\n",
    "            context = corpus[[1]][j]\n",
    "            df[i, I[[context]]] = 1\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "targetvars = paste(targetvars, collapse=\"+\")\n",
    "contextvars = paste(contextvars, collapse=\"+\")\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c31d819",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a00c1fae50278f1c71e3067241d99221",
     "grade": false,
     "grade_id": "cell-ce0c40a70749bcd9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### CBoW Model\n",
    "\n",
    "Now we are ready to train the CBoW version of the model. This code also outputs the sum of squared errors (SSE) that the model produces and the number of steps that it took to reach convergence. As the name suggests, to calculate the SSE we take the differences between the teacher and the activation of each output unit for all patterns, square them and then add. The network is reproducing the teacher patterns well when the SSE is close to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b945e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "NumHidden = 5\n",
    "set.seed(9)\n",
    "strformula = paste(targetvars, \"~\", contextvars)\n",
    "nn = neuralnet(as.formula(strformula), data=df, hidden=NumHidden, act.fct=\"logistic\", linear.output=FALSE)\n",
    "print (\"Error:\")\n",
    "print (nn$result.matrix[[1]])\n",
    "print(\"Number of steps:\")\n",
    "print (nn$result.matrix[[3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd611b45",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d2c52487c9f3b34b7f1274f98bbdbdbf",
     "grade": false,
     "grade_id": "cell-4a4cd75b7e5f9f23",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "And now we can plot the hierarchical cluster diagram of the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888fd72f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c4b69e95f70575b8f77bd3cc08a42ab8",
     "grade": false,
     "grade_id": "cell-ccace4cf7a2fd906",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "ws = data.frame(t(nn$weights[[1]][[2]][2:(NumHidden+1), 1:V]))\n",
    "rownames(ws) = vocab\n",
    "options(repr.plot.width=8, repr.plot.height=15)\n",
    "dist_mat <- dist(ws, method = 'euclidean')\n",
    "hclust_avg <- hclust(dist_mat, method = 'average')\n",
    "plot(as.dendrogram(hclust_avg), horiz=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091764bc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "59136faee4f7ced2dd9102947bc4d485",
     "grade": false,
     "grade_id": "cell-c81a59ee342735e3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Skipgram Model\n",
    "\n",
    "Now we will do the same for the skipgram version of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1aa506",
   "metadata": {},
   "outputs": [],
   "source": [
    "strformula = paste(contextvars, \"~\", targetvars)\n",
    "NumHidden = 5\n",
    "set.seed(9)\n",
    "nnSkipgram = neuralnet(as.formula(strformula), data=df, hidden=NumHidden, act.fct=\"logistic\", linear.output=FALSE)\n",
    "print (\"Error:\")\n",
    "print(nnSkipgram$result.matrix[[1]])\n",
    "print(\"Number of steps:\")\n",
    "print(nnSkipgram$result.matrix[[3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba212723",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7d66cec2f3d1ee5f9a47d8ec425f0771",
     "grade": false,
     "grade_id": "cell-1b48dce8417f9870",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "And now we can plot the hierachical cluster diagram for the skipgram version of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607372da",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bfbc1f9121165b0591c8cbb43728ecbe",
     "grade": false,
     "grade_id": "cell-36d94f58e9179f49",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "ws = data.frame(nnSkipgram$weights[[1]][[1]][2:(V+1),1:NumHidden])\n",
    "rownames(ws) = vocab\n",
    "options(repr.plot.width=8, repr.plot.height=15)\n",
    "dist_mat <- dist(ws, method = 'euclidean')\n",
    "hclust_avg <- hclust(dist_mat, method = 'average')\n",
    "plot(as.dendrogram(hclust_avg), horiz=TRUE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513da417",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e4c9b5991409a6a2b3a2663cb9277418",
     "grade": false,
     "grade_id": "cell-57d5d64e6fd049f2",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Exercise 1\n",
    "\n",
    "A major part of the work in applying neural networks to a problem is to decide on the parameters one will use. We will focus on two main ones for this assignment - the context size and the number of hidden units. \n",
    "\n",
    "The context size determines how many words to the left and right of the target word the models considers. The number of hidden units determines how many units are in the middle layer that maps the inputs to the outputs.  Changing these parameters impacts on the performance of the network.\n",
    "\n",
    "Using the code above, run the CBoW and Skipgram models with different values of the context size and different numbers hidden units. Specifically, consider context sizes of 1 or 2 and 5, 10 or 15 hidden units. Tabulate the error and the number of steps taken to converge. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ebed1d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "64162ed8bdb0006463cdd9967ae00585",
     "grade": true,
     "grade_id": "cell-af0e3583a8185545",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbec79a8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7d3a19743edc9b6834ecc492ba04f85f",
     "grade": false,
     "grade_id": "cell-8666ecc0d636fcb6",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Exercise 2\n",
    "\n",
    "Why are the CBoW error values different from the Skipgram values?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c8beaf",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2e1d37ab7366318219d5bbcfdafb5eaf",
     "grade": true,
     "grade_id": "cell-0dcd47375192099d",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158ada86",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5ccfb5fa63b7027fbdd43547ac4fcd73",
     "grade": true,
     "grade_id": "cell-eec81f37ad25ce0a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db69ef40",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "56b14a50fb7a5124073353863fcbed58",
     "grade": true,
     "grade_id": "cell-33bd381f3b1d4b35",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c662537b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d06f572e4051cbb55c871767c14788d3",
     "grade": false,
     "grade_id": "cell-28af7b4a92dbb298",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Exercise 4\n",
    "\n",
    "How do the error values typically change as a function of the number hidden units? Why would that be the case?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65117e7d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a743a24dddcc46ae41ebaf68bdb4abac",
     "grade": true,
     "grade_id": "cell-c846485559a44c69",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d45057",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9faeb9ce6478618992e0cf860c974cfd",
     "grade": false,
     "grade_id": "cell-15f395beaadc4481",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Exercise 5\n",
    "\n",
    "In general, the more hidden units you have the lower the error becomes. Does that mean that more hidden units are always better? \n",
    "\n",
    "Compare the hierarachical cluster digram of the CBoW model with a context size of 1 and 10 hidden units and with 100 hidden units. Pay particular attention to where \"tree\" is placed relative to the other nouns like \"box\" and \"house\".  Which makes the most sense? When will fewer hidden units be better?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a33d927",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "912c98f1a62a1ab93e2c7d89fcec85ec",
     "grade": true,
     "grade_id": "cell-c8bfe55656b9ae3b",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
